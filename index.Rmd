---
title: "Dumbbell Lift Classification"
author: "Michael Ching"
date: "January 19, 2015"
output: html_document
---

# Summary
This manuscript describes the process of classifying dumbbell lifts into different categories of how well the exercise was performed based on automated sensor data. A machine learning model was created using the random forests algorithm. Accuracy approached 100%, and the out of sample error rate was estimated to be 0.1%. 

## Data Source
Data were collected by Velloso et al. (2013), available here: http://groupware.les.inf.puc-rio.br/har. As described on the website: "Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)." I downloaded the data from the Coursera links as below. 

```{r, cache=TRUE}
library(RCurl)
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
timestamp()
x <- getURL(url_train)
train <- read.csv(textConnection(x), na.strings = c("NA", "#DIV/0!"))
x <- getURL(url_test)
test <- read.csv(textConnection(x), na.strings = c("NA", "#DIV/0!"))
rm(x)
```

## Exploring Data 
The full training data set contained `r nrow(train)` observations on `r ncol(train)` variables. The test data set contained `r nrow(test)` observations. Feature names included a variety of data from belt, arm, forearm, and dumbbell sensors.

```{r}
library(caret); library(randomForest)
```

## Data Processing
I split the provided training data into training and validation samples.
```{r}
set.seed(1234)
inTrain <- createDataPartition(y=train$classe, p=0.7, list=FALSE)
train <- train[inTrain,]
probe <- train[-inTrain,]
```

Because I planned to use random forests as a non-parametric method of creating a classification model, I assessed for the presence of "NA" observations and other observations that would not be handled appropriately by this algorithm. In most of the variables in which there were "NA", the "NA" values constituted the vast majority of the observations, and imputation would not be appropriate. I elected to remove these from the training data. 
```{r}
train_no_NAs <- sapply(train, function(x) !any(is.na(x)))
train_complete <- train[, train_no_NAs]
```

## Model Creation
To create the classification model, I used the `randomForest` function from the `randomForest` package. I elected not to use the first 7 features since they did not contain data that I felt would be relevant to prediction. That is, the row number, timestamps, subject ID, and whether it was the first observation of a new window of observations.

```{r, cache = TRUE}
set.seed(96825)
modfit <- randomForest(classe ~ ., data = train_complete[, -(1:7)])
```

## Cross Validation and Estimating Out of Sample Error Rate
The estimate of out of sample error rate is calculated internally via bootstrapping. That is, each tree is constructed from a random sample of the data (approximately 2/3 of the data). Each of the remaining left-out observations are predicted using the tree, and the probability of being in the most common "out of bag" category after the model is complete results in the out-of-bag error estimate of `r round(modfit$err.rate[nrow(modfit$err.rate)] * 100, 2)`%. (Reference: https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)

```{r}
print(modfit)
```

## Confusion Matrix
I estimated the accuracy of the model in predicting the class of barbell exercise by constructing confusion matrices. Two matrices were generated, one for predictions on the training data and one for predictions on the validation data.

### Training Sample Prediction
```{r}
predtrain <- predict(modfit, train_complete)
confusionMatrix(predtrain, train_complete$classe)
```

### Validation Sample Prediction
```{r}
predprobe <- predict(modfit, probe)
confusionMatrix(predtrain, train_complete$classe)
```

## Predictions on Test Data
Finally I used the model to predict the class for each of the test observations.

```{r}
predtest <- predict(modfit, test)
print(predtest)
```

I output these answers into text files for grading.
```{r}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("./answers/problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(predtest)
```

